{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification with a Neural Network\n",
    "***\n",
    "In this script, a neural network consisting of **one input layer (784 Neurons), three hidden layers (128/72/72 Neurons), and one output layer (10 Neurons)**. \n",
    "The input are the grayscale pixel values of 28x28 images of handwritten digits. The output neurons indicate to which of the 10 digit classes (0 - 9) the input is expected to belong. The network is trained in a classic stochastic gradient descent manner, with sigmoids as activation functions and the sum of squared errors of prediction (SSE) is the loss function which is optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01 # Learning rate\n",
    "#Initialise the five weight matrices (784/128/72 + Bias Unit)\n",
    "W1 = np.random.uniform(-1, 1, (785, 128))\n",
    "W2 = np.random.uniform(-1, 1, (129, 72))\n",
    "W3 = np.random.uniform(-1, 1, (73, 72))\n",
    "W4 = np.random.uniform(-1, 1, (73, 72))\n",
    "W5 = np.random.uniform(-1, 1, (73, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1/(1+np.exp(-x))\n",
    "def d_activation(x):\n",
    "    y = activation(x)\n",
    "    return y*(1-y)\n",
    "def d_error(y_hat, y):\n",
    "    return (y_hat-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forward pass consists of matrix-multiplication of the respective weight matrix and the previous output, followed by a pointwise activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    x = np.append(x, 1)\n",
    "    sum1 = np.dot(x, W1)\n",
    "    ac1 = activation(sum1)\n",
    "    ac1 = np.append(ac1, 1)\n",
    "    sum2 = np.dot(ac1, W2)\n",
    "    ac2 = activation(sum2)\n",
    "    ac2 = np.append(ac2, 1)\n",
    "    sum3 = np.dot(ac2, W3)\n",
    "    ac3 = activation(sum3)\n",
    "    ac3 = np.append(ac3, 1)\n",
    "    sum4 = np.dot(ac3, W4)\n",
    "    ac4 = activation(sum4)\n",
    "    ac4 = np.append(ac4, 1)\n",
    "    sum5 = np.dot(ac4, W5)\n",
    "    y_hat = activation(sum5)\n",
    "\n",
    "    return sum1, ac1, sum2, ac2, sum3, ac3, sum4, ac4, sum5, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the **delta functions** for the neurons (except for the input layer) which enables us to define a backward pass again in the form of a vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta5(sum5, y_hat, y_vector):\n",
    "    delta = np.zeros((10,))\n",
    "    for i, y in enumerate(y_vector):\n",
    "        delta[i] = d_error(y_hat[i], y)*d_activation(sum5[i])\n",
    "    return delta\n",
    "def calc_delta4(sum4, delta5):\n",
    "    delta = np.dot(W5[:-1][:], delta5)*d_activation(sum4)\n",
    "    return delta\n",
    "\n",
    "def calc_delta3(sum3, delta4):\n",
    "    delta = np.dot(W4[:-1][:], delta4)*d_activation(sum3)\n",
    "    return delta\n",
    "\n",
    "def calc_delta2(sum2, delta3):\n",
    "    delta = np.dot(W3[:-1][:], delta3)*d_activation(sum2)\n",
    "    return delta\n",
    "\n",
    "def calc_delta1(sum1, delta2):\n",
    "    delta = np.dot(W2[:-1][:], delta2)*d_activation(sum1)\n",
    "    return delta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using these delta functions, the backward pass is just the consecutive application of the outer product between the previous activation and the respective delta functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(x, ac1, ac2, ac3, ac4, sum1, sum2, sum3, sum4, sum5, y_hat, y):\n",
    "    x = np.append(x, 1)\n",
    "    \n",
    "    delta5 = calc_delta5(sum5, y_hat, y)\n",
    "    delta4 = calc_delta4(sum4, delta5)\n",
    "    delta3 = calc_delta3(sum3, delta4)    \n",
    "    delta2 = calc_delta2(sum2, delta3)    \n",
    "    delta1 = calc_delta1(sum1, delta2)\n",
    "    \n",
    "    dW5 = np.outer(ac4, delta5)  \n",
    "    dW4 = np.outer(ac3, delta4)      \n",
    "    dW3 = np.outer(ac2, delta3)  \n",
    "    dW2 = np.outer(ac1, delta2)\n",
    "    dW1 = np.outer(x, delta1)\n",
    "    \n",
    "    return dW1, dW2, dW3, dW4, dW5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the simplest method of updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(dW1, dW2, dW3, dW4, dW5):\n",
    "    global W1, W2, W3, W4, W5\n",
    "    W1 = W1 - eta*dW1\n",
    "    W2 = W2 - eta*dW2\n",
    "    W3 = W3 - eta*dW3\n",
    "    W4 = W4 - eta*dW4\n",
    "    W5 = W5 - eta*dW5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y(label):\n",
    "    return np.identity(10)[int(label)]\n",
    "def calc_y_hat(y_hat):\n",
    "    return np.argmax(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can import some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = genfromtxt('mnist_train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_test = genfromtxt('mnist_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = my_data[:, 1:]\n",
    "data_label = my_data[:, 0]\n",
    "data_test = my_data_test[:, 1:]\n",
    "data_test_label = my_data_test[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "***\n",
    "In the training phase, we run over the data set 50-times and update the weights after each forward/backward pass (stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(50):\n",
    "    for i in range(len(data_train)):\n",
    "        data_x = data_train[i, :]\n",
    "        data_y = calc_y(data_label[i])\n",
    "        sum1, ac1, sum2, ac2, sum3, ac3, sum4, ac4, sum5, y_hat = forward_pass(data_x)\n",
    "        dW1, dW2, dW3, dW4, dW5 = backward_pass(data_x, ac1, ac2, ac3, ac4, sum1, sum2, sum3, sum4, sum5, y_hat, data_y)\n",
    "        update_weights(dW1, dW2, dW3, dW4, dW5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can have a look how well our algorithm performs and create a confusion matrix to get a better overview over the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.8934\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43b049a278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = 0 \n",
    "conf = np.zeros((10, 10))\n",
    "for i in range(len(data_test)):\n",
    "    data_x = data_test[i, :]\n",
    "    label = data_test_label[i]\n",
    "    _, _, _, _, _, _, _, _, _, y_hat = forward_pass(data_x)\n",
    "    label_hat = calc_y_hat(y_hat)\n",
    "    conf[int(label)][label_hat] += 1\n",
    "    if label == label_hat:\n",
    "        corr += 1\n",
    "acc = corr / len(data_test)\n",
    "print('acc', acc)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(conf, cmap=plt.cm.jet)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
